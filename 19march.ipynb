{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da302cc-35ef-401a-b1d3-d328129358a6",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing?\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale features to a specific range, usually between 0 and 1. It helps in bringing all the features to a similar scale, which can be important for machine learning algorithms that are sensitive to the scale of features, like gradient descent-based algorithms.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset of house prices, and the feature 'square footage' ranges from 1000 to 3000 square feet. By applying Min-Max scaling, you can transform this range to [0, 1], where 1000 becomes 0, 3000 becomes 1, and values in between are scaled proportionally.\n",
    "\n",
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "The Unit Vector technique, also known as normalization, scales each data point in the dataset to have a unit norm (length). It's often used when the direction of the data points is more important than their absolute values. In contrast, Min-Max scaling focuses on bringing the features within a specific range.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset of product purchase history, and each data point represents a customer's purchase behavior as a vector. By applying the Unit Vector technique, you scale each customer's vector to have a length of 1, maintaining the direction of their behavior while eliminating the magnitude differences.\n",
    "\n",
    "##  Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction?\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional representation while preserving as much of the original data's variance as possible. It achieves this by identifying orthogonal axes (principal components) along which the data varies the most.\n",
    "\n",
    "Example:\n",
    "Suppose you have a dataset with many correlated features describing customer behavior. PCA can identify new dimensions (principal components) that capture the most variance in the data. You can then choose to retain a reduced number of principal components, effectively reducing the dimensionality of your data.\n",
    "\n",
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction?\n",
    "PCA can be used for feature extraction by identifying and creating new features that are linear combinations of the original features. These new features (principal components) are orthogonal and are ordered by the amount of variance they capture. Feature extraction with PCA is often used to reduce dimensionality and eliminate multicollinearity.\n",
    "\n",
    "Example:\n",
    "Imagine you have a dataset with various financial indicators (e.g., revenue, profit, expenses) for companies. Instead of using all these indicators as separate features, PCA could help extract a smaller number of principal components that represent the most significant patterns in the financial data.\n",
    "\n",
    "## Q5. How would you use Min-Max scaling to preprocess the data for a food delivery recommendation system?\n",
    "In a food delivery recommendation system, you might have features like price, rating, and delivery time, each with different scales. To ensure that these features have a consistent impact on the recommendation model, you would apply Min-Max scaling to bring them all into a similar range, such as [0, 1].\n",
    "\n",
    "## Q6. How would you use PCA to reduce the dimensionality of a dataset for predicting stock prices?\n",
    "For predicting stock prices using a dataset with numerous features, PCA could help reduce dimensionality by identifying the most influential patterns in the data. You would apply PCA to the dataset, retain a certain number of principal components (which capture most of the variance), and then use these reduced components for building your predictive model.\n",
    "\n",
    "## Q7. Perform Min-Max scaling to transform the values [1, 5, 10, 15, 20] to a range of -1 to 1.\n",
    "For Min-Max scaling to the range [-1, 1], the formula is:\n",
    "\n",
    "scaled_value = (x - min) / (max - min) * 2 - 1\n",
    "\n",
    "After applying this formula to each value:\n",
    "\n",
    "1 becomes -1\n",
    "5 becomes -0.5\n",
    "10 becomes 0\n",
    "15 becomes 0.5\n",
    "20 becomes 1\n",
    "## Q8. For a dataset containing the features [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "The number of principal components to retain depends on the desired balance between reducing dimensionality and preserving information. You would typically look at the cumulative explained variance ratio to decide. Let's assume after applying PCA, you find that 95% of the variance is captured by the first two principal components. In this case, you might choose to retain these two components to reduce the dataset's dimensionality while still preserving the majority of the original data's variability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22fc19-9cb5-4d65-99be-a73815b1de13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
